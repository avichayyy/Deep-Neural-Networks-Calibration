{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "# %matplotlib inline\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "# import torchaudio\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import gc\n",
    "from netcal.metrics import ECE\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAM_DPI = 90 # image quality of spectrograms\n",
    "DEFAULT_SAMPLE_RATE = 44100\n",
    "DEFAULT_HOPE_LENGHT = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "      <th>mids</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male_speech_and_man_speaking</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snare_drum</td>\n",
       "      <td>776</td>\n",
       "      <td>776</td>\n",
       "      <td>776</td>\n",
       "      <td>776</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laughter</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cello</td>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Trumpet</td>\n",
       "      <td>631</td>\n",
       "      <td>631</td>\n",
       "      <td>631</td>\n",
       "      <td>631</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fart</td>\n",
       "      <td>629</td>\n",
       "      <td>629</td>\n",
       "      <td>629</td>\n",
       "      <td>629</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Clarinet</td>\n",
       "      <td>592</td>\n",
       "      <td>592</td>\n",
       "      <td>592</td>\n",
       "      <td>592</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Piano</td>\n",
       "      <td>585</td>\n",
       "      <td>585</td>\n",
       "      <td>585</td>\n",
       "      <td>585</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Squeak</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Acoustic_guitar</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Saxophone</td>\n",
       "      <td>563</td>\n",
       "      <td>563</td>\n",
       "      <td>563</td>\n",
       "      <td>563</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Flute</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Coin_(dropping)</td>\n",
       "      <td>529</td>\n",
       "      <td>529</td>\n",
       "      <td>529</td>\n",
       "      <td>529</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Double_bass</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>527</td>\n",
       "      <td>527</td>\n",
       "      <td>527</td>\n",
       "      <td>527</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Female_speech_and_woman_speaking</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Violin_and_fiddle</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Applause</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bark</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            category  fname  labels  mids  path  class\n",
       "0       Male_speech_and_man_speaking    846     846   846   846    846\n",
       "1                         Snare_drum    776     776   776   776    776\n",
       "2                           Laughter    711     711   711   711    711\n",
       "3                    Electric_guitar    687     687   687   687    687\n",
       "4                              Cello    660     660   660   660    660\n",
       "5                            Trumpet    631     631   631   631    631\n",
       "6                               Fart    629     629   629   629    629\n",
       "7                           Clarinet    592     592   592   592    592\n",
       "8                              Piano    585     585   585   585    585\n",
       "9                             Squeak    583     583   583   583    583\n",
       "10                   Acoustic_guitar    581     581   581   581    581\n",
       "11                         Saxophone    563     563   563   563    563\n",
       "12                             Flute    531     531   531   531    531\n",
       "13                   Coin_(dropping)    529     529   529   529    529\n",
       "14                       Double_bass    528     528   528   528    528\n",
       "15                            Hi-hat    527     527   527   527    527\n",
       "16  Female_speech_and_woman_speaking    525     525   525   525    525\n",
       "17                 Violin_and_fiddle    518     518   518   518    518\n",
       "18                          Applause    502     502   502   502    502\n",
       "19                              Bark    500     500   500   500    500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_only_one_class(l):\n",
    "    if type(l) != str:\n",
    "        return l\n",
    "    return l.split(',')[0]\n",
    "base_dir = '/home/user_7428/databases/FSD'\n",
    "spectrogram_dir = f\"{base_dir}/spectograms\"\n",
    "df_dev = pd.read_csv(f\"{base_dir}/metadata/collection_dev.csv\")\n",
    "df_eval = pd.read_csv(f\"{base_dir}/metadata/collection_eval.csv\")\n",
    "df_dev['path'] = df_dev['fname'].apply(lambda file: f'dev_audio/{file}.wav')\n",
    "df_eval['path'] = df_eval['fname'].apply(lambda file: f'eval_audio/{file}.wav')\n",
    "df = pd.concat([df_dev, df_eval]).drop_duplicates().reset_index(drop=True)\n",
    "df['class'] = df['labels']\n",
    "df['category'] = df['labels'].apply(get_only_one_class)\n",
    "var = df.groupby('category').agg('count').sort_values('fname', ascending=False)\n",
    "K = 20\n",
    "top_K_classes = var.head(K).reset_index()\n",
    "df = df[df['category'].isin(top_K_classes['category'])]\n",
    "classes = []\n",
    "for l in df['category'].unique():\n",
    "    if type(l) != str:\n",
    "        continue\n",
    "    for c in l.split(','):\n",
    "        if c not in classes:\n",
    "            classes.append(c)\n",
    "classes_num = len(classes)\n",
    "CONSTANT_NUMBER_OF_SAMPLES = top_K_classes['fname'].min() * len(top_K_classes)\n",
    "top_K_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "matplotlib.pyplot.close()\n",
    "if not os.path.exists(spectrogram_dir):\n",
    "    os.mkdir(spectrogram_dir)\n",
    "for i, row in df.iterrows():\n",
    "    wav_file = f\"{base_dir}/{row['path']}\"\n",
    "    png_file = f\"{spectrogram_dir}/{row['path'].split('/')[1].replace('.wav', '.png')}\"\n",
    "    if os.path.exists(png_file):\n",
    "        continue\n",
    "    waveform, sample_rate = librosa.load(wav_file, sr=None)\n",
    "    # waveform = waveform.numpy()\n",
    "    fig, axes = plt.subplots(1, 1)\n",
    "    axes.specgram(waveform, Fs=sample_rate)\n",
    "    # f = plt.figure()\n",
    "    axes.axis('off')\n",
    "    # plt.show(block=False)\n",
    "    plt.savefig(f'{png_file}', dpi=SPECTROGRAM_DPI , bbox_inches='tight')\n",
    "    plt.cla()\n",
    "    plt.close(fig)\n",
    "    os.remove(wav_file)\n",
    "    del fig\n",
    "    del axes\n",
    "    del wav_file\n",
    "    gc.collect()\n",
    "    count += 1\n",
    "    # print(\"Created spectogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user_7428/databases/FSD/spectograms/156500.png: (519, 367)\n"
     ]
    }
   ],
   "source": [
    "def inspect_image_dimensions(image_dir, num_images=1):\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
    "    for img_path in image_paths[:num_images]:\n",
    "        with Image.open(img_path) as img:\n",
    "            print(f'{img_path}: {img.size}')\n",
    "inspect_image_dimensions(spectrogram_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "      <th>mids</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>category</th>\n",
       "      <th>png_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64760</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/64760.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/64760.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16399</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16399.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16399.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16401</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16401.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16401.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16402</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16402.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16402.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16404</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16404.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16404.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fname           labels      mids                 path            class  \\\n",
       "0  64760  Electric_guitar  /m/02sgy  dev_audio/64760.wav  Electric_guitar   \n",
       "1  16399  Electric_guitar  /m/02sgy  dev_audio/16399.wav  Electric_guitar   \n",
       "2  16401  Electric_guitar  /m/02sgy  dev_audio/16401.wav  Electric_guitar   \n",
       "3  16402  Electric_guitar  /m/02sgy  dev_audio/16402.wav  Electric_guitar   \n",
       "4  16404  Electric_guitar  /m/02sgy  dev_audio/16404.wav  Electric_guitar   \n",
       "\n",
       "          category               png_path  \n",
       "0  Electric_guitar  spectograms/64760.png  \n",
       "1  Electric_guitar  spectograms/16399.png  \n",
       "2  Electric_guitar  spectograms/16401.png  \n",
       "3  Electric_guitar  spectograms/16402.png  \n",
       "4  Electric_guitar  spectograms/16404.png  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['png_path'] = df['path'].apply(lambda path: path.replace('dev_audio/', 'spectograms/').replace('eval_audio/', 'spectograms/').replace('.wav', '.png'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 512, 384  # Adjusted dimensions based on inspection\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),  # Converts the image to tensor and scales pixel values to [0, 1]\n",
    "])\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, classes, transform, wanted_classes=None):\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.failed = 0\n",
    "        self.succeeded = 0\n",
    "        self.samples_per_label = None\n",
    "        if wanted_classes != None and type(wanted_classes) == dict:\n",
    "            self.samples_per_label = {label: 0 for label in wanted_classes}\n",
    "        for fname in os.listdir(image_dir):\n",
    "            if fname.endswith('.png'):\n",
    "                # try:\n",
    "                    current_class = df[df['png_path'] == f'spectograms/{fname}']['category']\n",
    "                    if len(current_class) == 0:\n",
    "                        continue\n",
    "                    current_class = current_class.values[0]\n",
    "                    label = classes.index(current_class)\n",
    "                    if wanted_classes != None and current_class not in wanted_classes:\n",
    "                        continue\n",
    "                    if wanted_classes != None and type(wanted_classes) == dict and self.samples_per_label[current_class] >= wanted_classes[current_class]:\n",
    "                        continue\n",
    "                    image_path = os.path.join(image_dir, fname)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(label)\n",
    "                    self.samples_per_label[current_class] += 1\n",
    "                    self.succeeded += 1\n",
    "                # except Exception as e:\n",
    "                #     self.failed += 1\n",
    "        print(f\"Created data loader. Failed: {self.failed}, Total: {self.failed + self.succeeded}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # return pil_to_tensor(image), label\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_and_train_loader(batch_size, wanted_classes=None, train_fraction=0.8):\n",
    "    dataset = SpectrogramDataset(spectrogram_dir, df, classes, transform, wanted_classes)\n",
    "    train_size = int(train_fraction * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * (img_height // 8) * (img_width // 8), 512)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, save_path):\n",
    "    if save_path != None:\n",
    "        print(f\"Checking if path {save_path} exists\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Model loaded from {save_path}')\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            return\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for _, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / 100:.4f}')\n",
    "    if save_path != None:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Model saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    confidences = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidences += list(probabilities.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ground_truth += list(labels.to('cpu').numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Total: {total}, Correct: {correct}. Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    confidences = np.array(confidences)\n",
    "    return ground_truth, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ece_and_acc(confidences, ground_truth):\n",
    "    correct = ground_truth == confidences.argmax(axis=1)\n",
    "    sum_true = np.sum(correct)\n",
    "    total = correct.size\n",
    "    acc = sum_true / total\n",
    "    n_bins = 10\n",
    "    ece = ECE(n_bins)\n",
    "    uncalibrated_score = ece.measure(confidences, ground_truth)\n",
    "    return uncalibrated_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_and_evaluate_model(\n",
    "        batch_size, \n",
    "        number_of_wanted_classes, \n",
    "        num_epochs, \n",
    "        save=False, \n",
    "        train_fraction=0.8, \n",
    "        wanted_classes=None, \n",
    "        initial_model=None\n",
    "    ):\n",
    "    result_dir = \"./results/experiment_one_data_variance\"\n",
    "    samples_variance = np.array([wanted_classes[key] for key in wanted_classes]).var() ** 0.5\n",
    "    samples_mean = np.array([wanted_classes[key] for key in wanted_classes]).mean()\n",
    "    ext = f\"_{number_of_wanted_classes}_classes_{batch_size}_batch_{num_epochs}_epochs_{samples_mean}_mean_{samples_variance}_variance\"\n",
    "    ece_file_name = f\"{result_dir}/ece{ext}.txt\"\n",
    "    if os.path.exists(ece_file_name):\n",
    "        return\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    # if wanted_classes == None:\n",
    "    #     wanted_classes = [i for i in range(number_of_wanted_classes)]\n",
    "    train_loader, test_loader = get_test_and_train_loader(batch_size, wanted_classes=wanted_classes, train_fraction=train_fraction)\n",
    "    model = copy.deepcopy(initial_model) \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    save_path = None\n",
    "    if save:\n",
    "        save_path=f\"{result_dir}/cnn_model{ext}.pth\"\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs, device, save_path)\n",
    "    ground_truth, confidences = evaluate_model(model, test_loader, device)\n",
    "    ece, acc = get_ece_and_acc(confidences, ground_truth)\n",
    "    result_dict = {\n",
    "        'ece': ece,\n",
    "        'acc': acc\n",
    "    }\n",
    "    with open(ece_file_name, 'wb') as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    return ece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Created data loader. Failed: 0, Total: 5000\n",
      "Epoch [1/1], Loss: 3.6277\n",
      "Total: 1000, Correct: 346. Accuracy of the model on the test images: 34.60%\n",
      "3.7682887362833544\n",
      "Created data loader. Failed: 0, Total: 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mcreate_train_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwanted_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwanted_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m change_variance(wanted_classes)\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mcreate_train_and_evaluate_model\u001b[0;34m(batch_size, number_of_wanted_classes, num_epochs, save, train_fraction, wanted_classes, initial_model)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[1;32m     28\u001b[0m     save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cnn_model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m ground_truth, confidences \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n\u001b[1;32m     31\u001b[0m ece, acc \u001b[38;5;241m=\u001b[39m get_ece_and_acc(confidences, ground_truth)\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, device, save_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     14\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mSpectrogramDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     37\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[0;32m---> 38\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:916\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    870\u001b[0m ):\n\u001b[1;32m    871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_k_random_numbers_whos_sum_is_const(K, bound, const=0):\n",
    "    result = np.random.randint(-bound, bound, K)\n",
    "    result[-1] -= result.sum()\n",
    "    return result\n",
    "\n",
    "def change_variance(wanted_classes, max_step_size=5):\n",
    "    steps = generate_k_random_numbers_whos_sum_is_const(K, max_step_size)\n",
    "    for i, l in enumerate(wanted_classes):\n",
    "        max_number_of_samples = top_K_classes[top_K_classes['category'] == l]['fname'].values[0]\n",
    "        step_size = steps[i]\n",
    "        direction = 1#(-1) ** i\n",
    "        step = step_size * direction\n",
    "        wanted_classes[l] += step\n",
    "        if wanted_classes[l] >= max_number_of_samples:\n",
    "            wanted_classes[l] = max_number_of_samples\n",
    "        if wanted_classes[l] <= 0:\n",
    "            wanted_classes[l] = min(np.random.randint(max_step_size), max_number_of_samples)\n",
    "\n",
    "INITIAL_NUMBER_OF_SAMPLES = int(0.5*int(CONSTANT_NUMBER_OF_SAMPLES / classes_num))\n",
    "wanted_classes = {l: INITIAL_NUMBER_OF_SAMPLES for l in classes}\n",
    "initial_model = CNN(num_classes=classes_num)\n",
    "for _ in range(30):\n",
    "    print(np.array([wanted_classes[key] for key in wanted_classes]).var() ** 0.5)\n",
    "    num_epochs = 100\n",
    "    batch_size = 32\n",
    "    create_train_and_evaluate_model(batch_size, classes_num, num_epochs, wanted_classes=wanted_classes, initial_model=initial_model)\n",
    "    change_variance(wanted_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
