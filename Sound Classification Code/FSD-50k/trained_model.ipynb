{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "# %matplotlib inline\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "# import torchaudio\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAM_DPI = 90 # image quality of spectrograms\n",
    "DEFAULT_SAMPLE_RATE = 44100\n",
    "DEFAULT_HOPE_LENGHT = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 51197, Number of classes: 365\n"
     ]
    }
   ],
   "source": [
    "def get_only_one_class(l):\n",
    "    if type(l) != str:\n",
    "        return l\n",
    "    return l.split(',')[0]\n",
    "base_dir = '/home/user_7428/databases/FSD'\n",
    "spectrogram_dir = f\"{base_dir}/spectograms\"\n",
    "df_dev = pd.read_csv(f\"{base_dir}/metadata/collection_dev.csv\")\n",
    "df_eval = pd.read_csv(f\"{base_dir}/metadata/collection_eval.csv\")\n",
    "df_dev['path'] = df_dev['fname'].apply(lambda file: f'dev_audio/{file}.wav')\n",
    "df_eval['path'] = df_eval['fname'].apply(lambda file: f'eval_audio/{file}.wav')\n",
    "df = pd.concat([df_dev, df_eval]).drop_duplicates().reset_index(drop=True)\n",
    "df['class'] = df['labels']\n",
    "df['category'] = df['labels'].apply(get_only_one_class)\n",
    "classes_num = len(df['category'].unique())\n",
    "print(f\"Number of samples: {len(df)}, Number of classes: {classes_num}\")\n",
    "classes = []\n",
    "for l in df['category'].unique():\n",
    "    if type(l) != str:\n",
    "        continue\n",
    "    for c in l.split(','):\n",
    "        if c not in classes:\n",
    "            classes.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "matplotlib.pyplot.close()\n",
    "if not os.path.exists(spectrogram_dir):\n",
    "    os.mkdir(spectrogram_dir)\n",
    "for i, row in df.iterrows():\n",
    "    wav_file = f\"{base_dir}/{row['path']}\"\n",
    "    png_file = f\"{spectrogram_dir}/{row['path'].split('/')[1].replace('.wav', '.png')}\"\n",
    "    if os.path.exists(png_file):\n",
    "        continue\n",
    "    waveform, sample_rate = librosa.load(wav_file, sr=None)\n",
    "    # waveform = waveform.numpy()\n",
    "    fig, axes = plt.subplots(1, 1)\n",
    "    axes.specgram(waveform, Fs=sample_rate)\n",
    "    # f = plt.figure()\n",
    "    axes.axis('off')\n",
    "    # plt.show(block=False)\n",
    "    plt.savefig(f'{png_file}', dpi=SPECTROGRAM_DPI , bbox_inches='tight')\n",
    "    plt.cla()\n",
    "    plt.close(fig)\n",
    "    os.remove(wav_file)\n",
    "    del fig\n",
    "    del axes\n",
    "    del wav_file\n",
    "    gc.collect()\n",
    "    count += 1\n",
    "    # print(\"Created spectogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user_7428/databases/FSD/spectograms/156500.png: (519, 367)\n"
     ]
    }
   ],
   "source": [
    "def inspect_image_dimensions(image_dir, num_images=1):\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
    "    for img_path in image_paths[:num_images]:\n",
    "        with Image.open(img_path) as img:\n",
    "            print(f'{img_path}: {img.size}')\n",
    "inspect_image_dimensions(spectrogram_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "      <th>mids</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>category</th>\n",
       "      <th>png_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64760</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/64760.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/64760.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16399</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16399.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16399.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16401</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16401.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16401.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16402</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16402.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16402.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16404</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>/m/02sgy</td>\n",
       "      <td>dev_audio/16404.wav</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>Electric_guitar</td>\n",
       "      <td>spectograms/16404.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fname           labels      mids                 path            class  \\\n",
       "0  64760  Electric_guitar  /m/02sgy  dev_audio/64760.wav  Electric_guitar   \n",
       "1  16399  Electric_guitar  /m/02sgy  dev_audio/16399.wav  Electric_guitar   \n",
       "2  16401  Electric_guitar  /m/02sgy  dev_audio/16401.wav  Electric_guitar   \n",
       "3  16402  Electric_guitar  /m/02sgy  dev_audio/16402.wav  Electric_guitar   \n",
       "4  16404  Electric_guitar  /m/02sgy  dev_audio/16404.wav  Electric_guitar   \n",
       "\n",
       "          category               png_path  \n",
       "0  Electric_guitar  spectograms/64760.png  \n",
       "1  Electric_guitar  spectograms/16399.png  \n",
       "2  Electric_guitar  spectograms/16401.png  \n",
       "3  Electric_guitar  spectograms/16402.png  \n",
       "4  Electric_guitar  spectograms/16404.png  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['png_path'] = df['path'].apply(lambda path: path.replace('dev_audio/', 'spectograms/').replace('eval_audio/', 'spectograms/').replace('.wav', '.png'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 512, 384  # Adjusted dimensions based on inspection\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),  # Converts the image to tensor and scales pixel values to [0, 1]\n",
    "])\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, classes, transform, wanted_classes=None):\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.failed = 0\n",
    "        self.succeeded = 0\n",
    "        for fname in os.listdir(image_dir):\n",
    "            if fname.endswith('.png'):\n",
    "                try:\n",
    "                    current_class = df[df['png_path'] == f'spectograms/{fname}']['category'].values[0]\n",
    "                    label = classes.index(current_class)\n",
    "                    if wanted_classes != None and label not in wanted_classes:\n",
    "                        continue\n",
    "                    image_path = os.path.join(image_dir, fname)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(label)\n",
    "                    self.succeeded += 1\n",
    "                except Exception as e:\n",
    "                    self.failed += 1\n",
    "        print(f\"Created data loader. Failed: {self.failed}, Total: {self.failed + self.succeeded}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # return pil_to_tensor(image), label\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_and_train_loader(batch_size, wanted_classes=None, train_fraction=0.8):\n",
    "    dataset = SpectrogramDataset(spectrogram_dir, df, classes, transform, wanted_classes)\n",
    "    train_size = int(train_fraction * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * (img_height // 8) * (img_width // 8), 512)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, save_path):\n",
    "    if save_path != None:\n",
    "        print(f\"Checking if path {save_path} exists\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Model loaded from {save_path}')\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            return\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / 100:.4f}')\n",
    "    if save_path != None:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Model saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    confidences = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidences += list(probabilities.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ground_truth += list(labels.to('cpu').numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Total: {total}, Correct: {correct}. Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    confidences = np.array(confidences)\n",
    "    return ground_truth, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ece(confidences, ground_truth):\n",
    "    correct = ground_truth == confidences.argmax(axis=1)\n",
    "    sum_true = np.sum(correct)\n",
    "    total = correct.size\n",
    "    acc = sum_true / total\n",
    "    n_bins = 10\n",
    "    ece = ECE(n_bins)\n",
    "    uncalibrated_score = ece.measure(confidences, ground_truth)\n",
    "    return uncalibrated_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data loader. Failed: 110, Total: 1326\n",
      "Checking if path ./results/cnn_model_2_classes_1_batch_5_epochs.pth exists\n",
      "Epoch [1/5], Loss: 2.4185\n",
      "Epoch [2/5], Loss: 1.6730\n",
      "Epoch [3/5], Loss: 1.0861\n",
      "Epoch [4/5], Loss: 1.4656\n",
      "Epoch [5/5], Loss: 0.8601\n",
      "Model saved to ./results/cnn_model_2_classes_1_batch_5_epochs.pth\n",
      "Total: 244, Correct: 234. Accuracy of the model on the test images: 95.90%\n",
      "Created data loader. Failed: 110, Total: 1486\n",
      "Checking if path ./results/cnn_model_3_classes_1_batch_5_epochs.pth exists\n",
      "Epoch [1/5], Loss: 6.6483\n",
      "Epoch [2/5], Loss: 4.0808\n",
      "Epoch [3/5], Loss: 3.7372\n",
      "Epoch [4/5], Loss: 3.1820\n",
      "Epoch [5/5], Loss: 2.7455\n",
      "Model saved to ./results/cnn_model_3_classes_1_batch_5_epochs.pth\n",
      "Total: 276, Correct: 237. Accuracy of the model on the test images: 85.87%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m number_of_wanted_classes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, classes_num):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num_epochs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m         \u001b[43mcreate_train_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_wanted_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36mcreate_train_and_evaluate_model\u001b[0;34m(batch_size, number_of_wanted_classes, num_epochs, train_fraction)\u001b[0m\n\u001b[1;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(result_dir)\n\u001b[1;32m      5\u001b[0m wanted_classes \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_wanted_classes)]\n\u001b[0;32m----> 6\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_test_and_train_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwanted_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwanted_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_fraction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m number_of_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mget_test_and_train_loader\u001b[0;34m(batch_size, wanted_classes, train_fraction)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_test_and_train_loader\u001b[39m(batch_size, wanted_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSpectrogramDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrogram_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwanted_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_fraction \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m      4\u001b[0m     test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mSpectrogramDataset.__init__\u001b[0;34m(self, image_dir, df, classes, transform, wanted_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m         current_class \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspectograms/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m         label \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39mindex(current_class)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m wanted_classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m wanted_classes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:5799\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5796\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   5797\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5799\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py:132\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_train_and_evaluate_model(batch_size, number_of_wanted_classes, num_epochs, save=False, train_fraction=0.8, min_acc=0.8):\n",
    "    result_dir = \"./results\"\n",
    "    ext = f\"_{number_of_wanted_classes}_classes_{batch_size}_batch_{num_epochs}_epochs\"\n",
    "    ece_file_name = f\"{result_dir}/ece{ext}.txt\"\n",
    "    if os.path.exists(ece_file_name):\n",
    "        return\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    wanted_classes = [i for i in range(number_of_wanted_classes)]\n",
    "    train_loader, test_loader = get_test_and_train_loader(batch_size, wanted_classes=wanted_classes, train_fraction=train_fraction)\n",
    "    model = CNN(num_classes=number_of_wanted_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    save_path = None\n",
    "    if save:\n",
    "        save_path=f\"{result_dir}/cnn_model{ext}.pth\"\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs, device, save_path)\n",
    "    ground_truth, confidences = evaluate_model(model, test_loader, device)\n",
    "    ece = get_ece(confidences, ground_truth)\n",
    "    with open(ece_file_name, 'w') as f:\n",
    "        f.write(f\"{ece}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_of_wanted_classes in range(0, classes_num, int(0.1 * len(classes_num))):\n",
    "    if number_of_wanted_classes == 0:\n",
    "        continue\n",
    "    for batch_size in [1, 4, 16, 32, 64]:\n",
    "        for num_epochs in [5, 10, 15, 20]:\n",
    "            create_train_and_evaluate_model(batch_size, number_of_wanted_classes, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
