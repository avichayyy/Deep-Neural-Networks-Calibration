{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "# %matplotlib inline\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from netcal.metrics import ECE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAM_DPI = 90 # image quality of spectrograms\n",
    "DEFAULT_SAMPLE_RATE = 44100\n",
    "DEFAULT_HOPE_LENGHT = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4680, Number of classes: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>unknown_yonatan_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio/b020_90_100.wav</td>\n",
       "      <td>beach</td>\n",
       "      <td>b020</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio/b020_110_120.wav</td>\n",
       "      <td>beach</td>\n",
       "      <td>b020</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio/b020_100_110.wav</td>\n",
       "      <td>beach</td>\n",
       "      <td>b020</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio/b020_40_50.wav</td>\n",
       "      <td>beach</td>\n",
       "      <td>b020</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio/b020_50_60.wav</td>\n",
       "      <td>beach</td>\n",
       "      <td>b020</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path  class unknown_yonatan_comment category\n",
       "0   audio/b020_90_100.wav  beach                    b020    beach\n",
       "1  audio/b020_110_120.wav  beach                    b020    beach\n",
       "2  audio/b020_100_110.wav  beach                    b020    beach\n",
       "3    audio/b020_40_50.wav  beach                    b020    beach\n",
       "4    audio/b020_50_60.wav  beach                    b020    beach"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/home/user_7428/databases/TUT-acoustic-scenes-2017-development'\n",
    "spectrogram_dir = f\"{base_dir}/spectograms\"\n",
    "df = pd.read_csv(f\"{base_dir}/meta.csv\")\n",
    "df['category'] = df['class']\n",
    "classes_num = len(df['class'].unique())\n",
    "classes = []\n",
    "for c in df['class'].unique():\n",
    "    classes.append(c)\n",
    "print(f\"Number of samples: {len(df)}, Number of classes: {classes_num}\")\n",
    "# df.groupby('class').agg('count')['path']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.close()\n",
    "if not os.path.exists(spectrogram_dir):\n",
    "    os.mkdir(spectrogram_dir)\n",
    "for i, row in df.iterrows():\n",
    "    png_file = f\"{spectrogram_dir}/{row['path'].split('/')[1].replace('.wav', '.png')}\"\n",
    "    if os.path.exists(png_file):\n",
    "        continue\n",
    "    waveform, sample_rate = librosa.load(f\"{base_dir}/{row['path']}\", sr=None)\n",
    "    waveform = waveform.numpy()\n",
    "    fig, axes = plt.subplots(1, 1)\n",
    "    axes.specgram(waveform[0], Fs=sample_rate)\n",
    "    # f = plt.figure()\n",
    "    axes.axis('off')\n",
    "    # plt.show(block=False)\n",
    "    plt.savefig(f'{png_file}', dpi=SPECTROGRAM_DPI , bbox_inches='tight')\n",
    "    plt.cla()\n",
    "    plt.close(fig)\n",
    "    # if i == 1000:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user_7428/databases/TUT-acoustic-scenes-2017-development/spectograms/b047_30_40.png: (519, 367)\n"
     ]
    }
   ],
   "source": [
    "def inspect_image_dimensions(image_dir, num_images=1):\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
    "    for img_path in image_paths[:num_images]:\n",
    "        with Image.open(img_path) as img:\n",
    "            print(f'{img_path}: {img.size}')\n",
    "inspect_image_dimensions(spectrogram_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        spectograms/b020_90_100.png\n",
       "1       spectograms/b020_110_120.png\n",
       "2       spectograms/b020_100_110.png\n",
       "3         spectograms/b020_40_50.png\n",
       "4         spectograms/b020_50_60.png\n",
       "                    ...             \n",
       "4675      spectograms/b081_50_60.png\n",
       "4676      spectograms/b081_60_70.png\n",
       "4677    spectograms/b081_100_110.png\n",
       "4678    spectograms/b081_110_120.png\n",
       "4679    spectograms/b081_120_130.png\n",
       "Name: png_path, Length: 4680, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['png_path'] = df['path'].apply(lambda path: path.replace('audio/', 'spectograms/').replace('.wav', '.png'))\n",
    "df['png_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 512, 384  # Adjusted dimensions based on inspection\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),  # Converts the image to tensor and scales pixel values to [0, 1]\n",
    "])\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, classes, transform, wanted_classes=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        if wanted_classes == None:\n",
    "            self.labels = [classes.index(df[df['png_path'] == f'spectograms/{fname}']['category'].values[0]) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
    "            self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
    "        else:\n",
    "            self.labels = []\n",
    "            self.image_paths = []\n",
    "            for fname in os.listdir(image_dir):\n",
    "                if not fname.endswith('.png'):\n",
    "                    continue\n",
    "                full_image_path = os.path.join(image_dir, fname)\n",
    "                label = classes.index(df[df['png_path'] == f'spectograms/{fname}']['category'].values[0])    \n",
    "                if label not in wanted_classes:\n",
    "                    continue\n",
    "                self.image_paths.append(full_image_path)\n",
    "                self.labels.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # return pil_to_tensor(image), label\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_and_train_loader(batch_size, wanted_classes=None, train_fraction=0.8):\n",
    "    dataset = SpectrogramDataset(spectrogram_dir, df, classes, transform, wanted_classes)\n",
    "    train_size = int(train_fraction * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * (img_height // 8) * (img_width // 8), 512)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, save_path):\n",
    "    if save_path != None:\n",
    "        print(f\"Checking if path {save_path} exists\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Model loaded from {save_path}')\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            return\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / 100:.4f}')\n",
    "    if save_path != None:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Model saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    confidences = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidences += list(probabilities.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ground_truth += list(labels.to('cpu').numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Total: {total}, Correct: {correct}. Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    confidences = np.array(confidences)\n",
    "    return ground_truth, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ece(confidences, ground_truth):\n",
    "    correct = ground_truth == confidences.argmax(axis=1)\n",
    "    sum_true = np.sum(correct)\n",
    "    total = correct.size\n",
    "    acc = sum_true / total\n",
    "    n_bins = 10\n",
    "    ece = ECE(n_bins)\n",
    "    uncalibrated_score = ece.measure(confidences, ground_truth)\n",
    "    return uncalibrated_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_and_evaluate_model(batch_size, number_of_wanted_classes, num_epochs, save=False, train_fraction=0.8):\n",
    "    result_dir = \"./results\"\n",
    "    ext = f\"_{number_of_wanted_classes}_classes_{batch_size}_batch_{num_epochs}_epochs\"\n",
    "    ece_file_name = f\"{result_dir}/ece{ext}.txt\"\n",
    "    if os.path.exists(ece_file_name):\n",
    "        return\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    wanted_classes = [i for i in range(number_of_wanted_classes)]\n",
    "    train_loader, test_loader = get_test_and_train_loader(batch_size, wanted_classes=wanted_classes, train_fraction=train_fraction)\n",
    "    model = CNN(num_classes=number_of_wanted_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    save_path = None\n",
    "    if save:\n",
    "        save_path=f\"{result_dir}/cnn_model{ext}.pth\"\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs, device, save_path)\n",
    "    ground_truth, confidences = evaluate_model(model, test_loader, device)\n",
    "    ece = get_ece(confidences, ground_truth)\n",
    "    with open(ece_file_name, 'w') as f:\n",
    "        f.write(f\"{ece}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9], Loss: 2.7905\n",
      "Epoch [2/9], Loss: 1.6113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m number_of_wanted_classes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, classes_num):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num_epochs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mcreate_train_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_wanted_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[142], line 17\u001b[0m, in \u001b[0;36mcreate_train_and_evaluate_model\u001b[0;34m(batch_size, number_of_wanted_classes, num_epochs, save, train_fraction)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[1;32m     16\u001b[0m     save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cnn_model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m ground_truth, confidences \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n\u001b[1;32m     19\u001b[0m ece \u001b[38;5;241m=\u001b[39m get_ece(confidences, ground_truth)\n",
      "Cell \u001b[0;32mIn[139], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, device, save_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for number_of_wanted_classes in range(0, classes_num, int(0.1 * len(classes_num))):\n",
    "    if number_of_wanted_classes == 0:\n",
    "        continue\n",
    "    for batch_size in [1, 4, 16, 32, 64]:\n",
    "        for num_epochs in [5, 10, 15, 20]:\n",
    "            create_train_and_evaluate_model(batch_size, number_of_wanted_classes, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
